# Dimas Portofolio
  Example Data Science protfolio
  
# [Project 1 : Image Classification: Image Case](https://github.com/terkoizmy/Image-Classification)
* Mount Google Drive to colab
* CNN model
* Test Prediction image

# [Project 2 : Sentiment Analys Classifier: NLP Case](https://github.com/terkoizmy/TheSocialDilemma)
* Classifier Sentiment Analys using Bert
* Mount to Google Drive for Save Model
* Encode and tokenizer text then use Tensor
* Confussion Matrix (Accuracy , precission, Recall)

#  [Project 3 : Titanic: Classification Case](https://github.com/terkoizmy/Titanic)
## Train for Submission
* Data Processing (Cleaning, Label Encoder, Data Scaler)
* classification model (Random forest, XGBoost, CatBoost)
* Save and Load model to drive
* Submission Score (0.77751) with XGBoost
## Explore Data Analys
  Exploratory Analysis of the passengers onboard RMS Titanic using Pandas and Seaborn visualization
  
#  [Project 4 : House Sell Price: Regressor case](https://github.com/terkoizmy/House-Price-Sell)
## Train for Submission
* Dealing with missing data (drop column and fill)
* Check a Correlation data
* Drop Outlier data
* Get dummies data 
* Scaling data (Normalization)
* Modeling With ANN
* Mount to drive
## Explore Data Analys
  Exploratory Analysis the correlation of column onboard House Price Sell case using Pandas and Seaborn visualization
  
#  [Project 5 : Retail: EDA case](https://github.com/terkoizmy/Retail)
## Explore Data Analys
  Simple Exploratory Analysis to find new insight from a data using Pandas and Seaborn visualization
  
#  [Project 6 : Tweet Disaster: NLP Case](https://github.com/terkoizmy/TweetDisaster)
## Basic NLP
* Text processing(lemmatizer & Stop wrord)
* Bag Of word
* modeling (xgboost & random forest)
## Modeling LSTM
* Text processing(lemmatizer & Stop wrord)
* Embedding text with glove (word2vec)
* Using LSTM layer for deep learning train model

#  [Project 7 : Population: Forcasting Case](https://github.com/terkoizmy/Population)
## forecasting model
* Data Processing (data cleaning, split data)
* Assumption check and prosses (differencing, ADF-test, Ljung-Box) 
* create Sarima model with hyper tunning
* check accuracy with MAE and MAPE
